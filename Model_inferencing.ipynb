{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af552804-c278-459d-9c74-7cd95e82d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "from tqdm import tqdm \n",
    "#from llama_cpp import Llama\n",
    "from transformers import LlamaTokenizer, MistralForCausalLM\n",
    "import bitsandbytes, flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "786eaca7-a587-46fd-80fa-bdbd556efca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataset = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e41a21ba-6f60-47d0-94ad-6151463c06bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 287113\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 13368\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['article', 'highlights', 'id'],\n",
       "        num_rows: 11490\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddefebe-feb7-466c-90a8-fc54fa807674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a931ad72-594d-4b4c-aa4e-962547cdfe44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': 'MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I probably had a 30-, 35-foot free fall. And there\\'s cars in the water, there\\'s cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge. They were yelling, screaming, bleeding. I think there were some broken bones.\"  Watch a driver describe his narrow escape » . At home when he heard about the disaster, Dr. John Hink, an emergency room physician, jumped into his car and rushed to the scene in 15 minutes. He arrived at the south side of the bridge, stood on the riverbank and saw dozens of people lying dazed on an expansive deck. They were in the middle of the Mississippi River, which was churning fast, and he had no way of getting to them. He went to the north side, where there was easier access to people. Ambulances were also having a hard time driving down to the river to get closer to the scene. Working feverishly, volunteers, EMTs and other officials managed to get 55 people into ambulances in less than two hours. Occasionally, a pickup truck with a medic inside would drive to get an injured person and bring him back up even ground, Hink told CNN. The rescue effort was controlled and organized, he said; the opposite of the lightning-quick collapse. \"I could see the whole bridge as it was going down, as it was falling,\" Babineau said. \"It just gave a rumble real quick, and it all just gave way, and it just fell completely, all the way to the ground. And there was dust everywhere and it was just like everyone has been saying: It was just like out of the movies.\" Babineau said the rear of his pickup truck was dangling over the edge of a broken-off section of the bridge. He said several vehicles slid past him into the water. \"I stayed in my car for one or two seconds. I saw a couple cars fall,\" he said. \"So I stayed in my car until the cars quit falling for a second, then I got out real quick, ran in front of my truck -- because behind my truck was just a hole -- and I helped a woman off of the bridge with me. \"I just wanted off the bridge, and then I ran over to the school bus. I started grabbing kids and handing them down. It was just complete chaos.\" He said most of the children were crying or screaming. He and other rescuers set them on the ground and told them to run to the river bank, but a few needed to be carried because of their injuries.  See rescuers clamber over rubble » . Babineau said he had no rescue training. \"I just knew what I had to do at the moment.\" Melissa Hughes, 32, of Minneapolis, told The Associated Press that she was driving home when the western edge of the bridge collapsed under her. \"You know that free-fall feeling? I felt that twice,\" Hughes said. A pickup landed on top of her car, but she was not hurt. \"I had no idea there was a vehicle on my car,\" she told AP. \"It\\'s really very surreal.\" Babineau told the Minneapolis Star-Tribune: \"On the way down, I thought I was dead. I literally thought I was dead. \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half. It was folded in half, and I can\\'t believe I\\'m alive.\"  See and hear eyewitness accounts » . Bernie Toivonen told CNN\\'s \"American Morning\" that his vehicle was on a part of the bridge that ended up tilted at a 45-degree angle. \"I knew the deck was going down, there was no question about it, and I thought I was going to die,\" he said. After the bridge settled and his car remained upright, \"I just put in park, turned the key off and said, \\'Oh, I\\'m alive,\\' \" he said. E-mail to a friend .',\n",
       " 'highlights': 'NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .',\n",
       " 'id': '06352019a19ae31e527f37f7571c6dd7f0c5da37'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_dataset[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac439c63-7496-4b35-a0ea-499e33d7fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Retrieve the dataset with 2000 data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab2f442a-1240-43d2-9a58-2757b7e11816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dataset(news_dataset,rows=2000):\n",
    "    article=[]\n",
    "    highlights=[]\n",
    "    \n",
    "    for articles in range(rows):\n",
    "        article.append(news_dataset[\"train\"][articles][\"article\"])\n",
    "        highlights.append(news_dataset[\"train\"][articles][\"highlights\"])\n",
    "        \n",
    "    extracted_info = pd.DataFrame({\n",
    "    'article': article,\n",
    "    'highlights': highlights})\n",
    "    \n",
    "    return extracted_info\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d4fa822-d4c9-4729-abf4-85440e9aa3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_dataframe=retrieve_dataset(news_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414b986c-b289-4901-81e2-7904d303ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the First LLM \"microsoft/Phi-3.5-MoE-instruct\" for extractive summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18df444d-3d7a-49bb-b0a9-f731bd350771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46557fc-9f95-4c00-88a6-7dd59bca839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(0) \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained( \n",
    "    \"microsoft/Phi-3.5-MoE-instruct\",  \n",
    "    device_map=\"cuda\",  \n",
    "    torch_dtype=\"auto\",  \n",
    "    trust_remote_code=False,  \n",
    ") \n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f25cd7f-41fa-4250-b039-a06f44024948",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    ##\"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6970e9b-089f-416f-a414-a3f427b30439",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_phi=[]\n",
    "for i in tqdm(range(len(news_dataframe[:310]))):\n",
    "    prompt_phi= f\"\"\" You are a helpful AI assistant that summarizes news articles accurately and concisely.\n",
    "                \n",
    "                 Given the article, generate a detailed 7-8 lines extractive summary in the form of a Python string.\n",
    "                 Never ever explain yourself,just give me the summary. \n",
    "                 Ensure the summary captures all key points and details from the article.\n",
    "                 DO NOT CONTINUE GENERATION AFTER GIVING THE OUTPUT.\n",
    "\n",
    "                 Here is my article:\n",
    "                 article: {news_dataframe.iloc[i]['article']} \\n\n",
    "                \n",
    "\n",
    "                 \"\"\"\n",
    "    messages = [ {\"role\": \"user\", \"content\" : prompt_phi } ]\n",
    "\n",
    "    output = pipe(messages, **generation_args) \n",
    "    \n",
    "    summary_phi.append(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb0c0f-3f40-4e41-949d-c0661d582553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the list of elements in Summary_Phi to a dataframe \n",
    "df_1 = pd.DataFrame(summary_phi, columns=[\"Summary_Phi\"])\n",
    "\n",
    "# Save the DataFrame as a CSV file without the index\n",
    "df_1.to_csv(\"summary_phi.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d33f24-2536-4ef2-85e4-fdbf1c54fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run the Second LLM \"NousResearch/Hermes-2-Pro-Mistral-7B-GGUF\" for abstractive summarization using Llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de3ad0e3-8d72-4115-b161-3fa67cac722a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.random.manual_seed(0) \n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('NousResearch/Hermes-2-Pro-Mistral-7B', trust_remote_code=False)\n",
    "model = MistralForCausalLM.from_pretrained(\n",
    "    \"NousResearch/Hermes-2-Pro-Mistral-7B\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": \"cuda:0\"}, \n",
    "    load_in_8bit=True,  \n",
    "    load_in_4bit=False,\n",
    "    use_flash_attention_2=False,\n",
    "    trust_remote_code=False\n",
    "    \n",
    ")\n",
    "print(next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b948834-67d4-42c7-ba57-9b3d09f173fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    ##\"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee875df-965a-4b36-b854-26c4160d7daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_mistral=[]\n",
    "for i in tqdm(range(len(news_dataframe[:310]))):\n",
    "    prompt_mistral= f\"\"\" You are a helpful AI assistant that summarizes news articles accurately and concisely.\n",
    "                \n",
    "                 Given the article, generate a detailed 7-8 lines abstract summary in the form of a Python string.\n",
    "                 Never ever explain yourself,just give me the summary. \n",
    "                 Ensure the summary captures all key points and details from the article.\n",
    "                 DO NOT CONTINUE GENERATION AFTER GIVING THE OUTPUT.\n",
    "\n",
    "                 Here is my article:\n",
    "                 article: {news_dataframe.iloc[i]['article']} \\n\n",
    "                \n",
    "\n",
    "                 \"\"\"\n",
    "    messages = [ {\"role\": \"user\", \"content\" : prompt_mistral } ]\n",
    "\n",
    "    output = pipe(messages, **generation_args) \n",
    "    \n",
    "    summary_mistral.append(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9aa2741-d021-4ce4-81ca-30546087f115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the list of elements in Summary_mistral to a dataframe \n",
    "df_2 = pd.DataFrame(summary_mistral, columns=[\"Summary_mistral\"])\n",
    "\n",
    "# Save the DataFrame as a CSV file without the index\n",
    "df_2.to_csv(\"summary_mistral.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e1760d-2f96-497a-a16f-9bee9baa458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deepseek-ai/DeepSeek-R1-Distill-Qwen-14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bae08e-26f4-4caa-a3e9-7768bfe6170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(0) \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\",\n",
    "     torch_dtype=\"auto\", \n",
    "     device_map={\"\": \"cuda:0\"}, \n",
    "     trust_remote_code=True\n",
    "    \n",
    ")\n",
    "print(next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6c8ff-a96d-4ba2-b57c-495c96c5ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"return_full_text\": False, \n",
    "    ##\"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "} \n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38b748-9e07-47b4-9369-e111d3e35dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_deepseek_qwen=[]\n",
    "for i in tqdm(range(len(news_dataframe[:310]))):\n",
    "    prompt_deepseek_qwen= f\"\"\" You are a helpful AI assistant that summarizes news articles accurately and concisely.\n",
    "                \n",
    "                 Given the article, generate a detailed 7-8 lines abstract summary in the form of a Python string.\n",
    "                 Never ever explain yourself,just give me the summary. \n",
    "                 Ensure the summary captures all key points and details from the article.\n",
    "                 DO NOT CONTINUE GENERATION AFTER GIVING THE OUTPUT.\n",
    "\n",
    "                 Here is my article:\n",
    "                 article: {news_dataframe.iloc[i]['article']} \\n\n",
    "                \n",
    "\n",
    "                 \"\"\"\n",
    "    messages = [ {\"role\": \"user\", \"content\" : prompt_deepseek_qwen } ]\n",
    "\n",
    "    output = pipe(messages, **generation_args) \n",
    "    \n",
    "    summary_deepseek_qwen.append(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai_env)",
   "language": "python",
   "name": "genai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
